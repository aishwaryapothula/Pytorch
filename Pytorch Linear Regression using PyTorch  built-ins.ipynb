{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs and targets\n",
    "# inputs (temp, rainfall, humidity)(obervations x each variable-temp,rain.humidity)\n",
    "# targets (apples, oranges) (outputs per observation x outputs-apples,oranges)\n",
    "\n",
    "inputs = torch.tensor([[73, 67, 43],\n",
    "                       [91, 88, 64],\n",
    "                       [87, 134, 58],\n",
    "                       [102, 43, 37],\n",
    "                       [69, 96, 70], \n",
    "                       [73, 67, 43],\n",
    "                       [91, 88, 64],\n",
    "                       [87, 134, 58],\n",
    "                       [102, 43, 37],\n",
    "                       [69, 96, 70],\n",
    "                       [73, 67, 43],\n",
    "                       [91, 88, 64],\n",
    "                       [87, 134, 58],\n",
    "                       [102, 43, 37],\n",
    "                       [69, 96, 70]], dtype = torch.float32)\n",
    "\n",
    "targets = torch.tensor([[56, 70],\n",
    "                        [81, 101],\n",
    "                        [119, 133],\n",
    "                        [22, 37],\n",
    "                        [103, 119],\n",
    "                        [56, 70],\n",
    "                        [81, 101],\n",
    "                        [119, 133],\n",
    "                        [22, 37],\n",
    "                        [103, 119],\n",
    "                        [56, 70],\n",
    "                        [81, 101],\n",
    "                        [119, 133],\n",
    "                        [22, 37],\n",
    "                        [103, 119]], dtype = torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a TensorDataset which allows access to rows as tuples\n",
    "# creating a DataLoader to slipt data into batches for training.\n",
    "# DataLoader also provides utilities such as shuffling and sampling\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining training dataset\n",
    "\n",
    "train_ds = TensorDataset(inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 69.,  96.,  70.],\n",
       "         [ 87., 134.,  58.],\n",
       "         [102.,  43.,  37.],\n",
       "         [ 87., 134.,  58.],\n",
       "         [ 69.,  96.,  70.]]), tensor([[103., 119.],\n",
       "         [119., 133.],\n",
       "         [ 22.,  37.],\n",
       "         [119., 133.],\n",
       "         [103., 119.]])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defining DataLoader to define batch size etc during training\n",
    "# train_dl contains 5 observations and corressponding target tuples \n",
    "# DataLoader combines a dataset and a sampler. Provides iterators over the dataset\n",
    "# shuffle shuffles the data in the dataset before each epoch\n",
    "# next(iter()) returns the next batch of obervations and corressponding targets from train_ds\n",
    "\n",
    "batch_size = 5\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle = True)\n",
    "next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.5101, -0.4320, -0.4786],\n",
      "        [-0.3628,  0.1757, -0.4952]], requires_grad=True)\n",
      "torch.Size([2, 3])\n",
      "Parameter containing:\n",
      "tensor([-0.0137, -0.1904], requires_grad=True)\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# we can use nn.linear instead of initializing weights and biases manually y=wx+b and w,b are randomly initialized\n",
    "\n",
    "model = nn.Linear(3, 2) # 3 factors and 2 outputs\n",
    "print(model.weight)\n",
    "print(model.weight.size())\n",
    "print(model.bias)\n",
    "print(model.bias.size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of manually updating weights and biases optimizers(can choose) can be used\n",
    "\n",
    "opt = torch.optim.SGD(model.parameters(), lr = 1e-5) # lr is learning rate to update w,b after calculating loss and gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27471.2383, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Loss function. Mean Squared Error calculation is embedded in mse_loss function. Can choose other loss calculations too\n",
    "\n",
    "import torch.nn.functional as F\n",
    "loss_fn = F.mse_loss\n",
    "loss = loss_fn(model(inputs), targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model\n",
    "# using a function called fit() to train the model over a specific number of epochs\n",
    "# it needs as inputs the number of epochs, model, loss function and optimiser\n",
    "# defining the fit() function\n",
    "\n",
    "def fit(num_epochs, model, loss_fn, opt):\n",
    "    for epoch in range(num_epochs):\n",
    "        for xb, yb, in train_dl: # train_dl is 1 batch\n",
    "            # Generating/Sampling predictions\n",
    "            pred = model(xb)# xb obervation of the three factors\n",
    "            loss = loss_fn(pred, yb)# yb corressponding output-yeilds of apples and oranges\n",
    "            # Gradient descent\n",
    "            loss.backward()\n",
    "            opt.step() # calling opt to update weights and biases\n",
    "            opt.zero_grad() # making the gradients zero after the iteration as PyTorch accumulates gradients\n",
    "        print(\"Training Loss: \", loss_fn(model(inputs), targets))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:  tensor(8670.6465, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(2865.5603, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(1108.7699, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(551.1914, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(383.3872, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(320.0350, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(295.6017, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(281.0715, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(271.5856, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(262.4278, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(254.6761, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(247.4583, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(240.1487, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(233.0982, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(225.9633, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(219.1916, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(212.7139, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(206.6872, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(200.8810, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(195.3519, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(189.7325, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(184.3902, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(179.3789, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(174.4022, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(169.6445, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(165.1158, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(160.8451, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(156.8577, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(152.2322, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(148.3408, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(144.5649, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(140.9448, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(137.4275, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(134.0303, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(130.8756, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(127.6174, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(124.4672, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(121.4477, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(118.6265, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(115.9843, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(113.5012, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(110.7650, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(108.0462, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(105.7787, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(103.3602, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(101.1099, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(98.9528, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(96.8573, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(94.8161, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(92.8354, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(90.9349, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(89.2337, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(87.3585, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(85.6348, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(83.9695, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(82.3669, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(80.8232, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(79.3155, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(77.8903, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(76.5026, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(75.1294, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(73.8104, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(72.4929, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(71.2759, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(70.1020, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(68.9361, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(67.7876, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(66.7220, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(65.6272, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(64.6163, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(63.6367, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(62.6484, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(61.6991, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(60.7705, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(59.8918, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(59.0333, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(58.2936, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(57.3961, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(56.5759, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(55.8516, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(55.1157, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(54.3666, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(53.6718, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(53.0371, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(52.4038, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(51.6591, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(51.0237, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(50.4193, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(49.8213, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(49.2340, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(48.6697, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(48.1668, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(47.6025, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(47.0721, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(46.5161, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(46.0369, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(45.5221, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(45.0471, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(44.5648, grad_fn=<MseLossBackward>)\n",
      "Training Loss:  tensor(44.1292, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# training the model for 100 epochs\n",
    "\n",
    "fit(100, model, loss_fn, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making predictions after training. Passing the same input dataset as testset\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
